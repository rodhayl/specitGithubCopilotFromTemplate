/**
 * TestCommand â€” `/test` command definition
 *
 * Registers the `/test` slash command in the Docu extension. When invoked, it
 * orchestrates a full run of extension scenarios and streams live results to the
 * VS Code Copilot Chat window. Logs are automatically saved to `.docu/test-results/`.
 *
 * Sub-commands
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 *   /test               Show this help text
 *   /test list          List every available scenario and its group
 *   /test quick         System + template + command scenarios (no LLM, fast)
 *   /test system        Sanity checks: agents, state, template count
 *   /test templates     Render every built-in template
 *   /test commands      All command-routing scenarios
 *   /test agents        All 6 agents via LLM (requires a model to be selected)
 *   /test workflow      Phase-tracking + workflow shortcut commands
 *   /test full          Everything â€” system + templates + commands + agents + workflow
 *   /test <id>          Run a single scenario by exact ID, e.g. /test agent:prd-creator
 */

import { CommandDefinition, ParsedCommand, CommandContext, CommandResult } from './types';
import { AgentManager } from '../agents/AgentManager';
import { TemplateService } from '../templates/TemplateService';
import { WorkflowStateManager } from '../state/WorkflowStateManager';
import { CommandRouter } from './CommandRouter';
import { TestRunner } from './TestRunner';
import { TestLogger, ScenarioResult, TestRunResult } from './TestLogger';

// â”€â”€â”€ Public factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export function createTestCommandDefinition(
    agentManager: AgentManager,
    templateService: TemplateService,
    workflowStateManager: WorkflowStateManager,
    commandRouter: CommandRouter
): CommandDefinition {
    const runner = new TestRunner(agentManager, templateService, workflowStateManager, commandRouter);

    return {
        name: 'test',
        description: 'Run extension test scenarios â€” agents, commands, templates, workflow, and e2e file creation',
        usage: '/test [quick|full|system|templates|commands|agents|workflow|e2e|list|<scenario-id>]',
        examples: [
            '/test quick',
            '/test full',
            '/test agents',
            '/test commands',
            '/test templates',
            '/test workflow',
            '/test e2e',
            '/test system',
            '/test list',
            '/test e2e:prd-creation',
            '/test agent:prd-creator',
            '/test system:agent-registration',
        ],

        handler: async (parsedCommand: ParsedCommand, context: CommandContext): Promise<CommandResult> => {
            // Derive sub-command from subcommand field or first positional argument
            const sub = (parsedCommand.subcommand ?? parsedCommand.arguments[0] ?? '').trim();

            // â”€â”€ /test list â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (sub === 'list') {
                return handleList(runner, context);
            }

            // â”€â”€ /test (no sub-command) or /test help â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (!sub || sub === 'help') {
                context.stream.markdown(HELP_TEXT);
                return { success: true, message: 'Test help displayed' };
            }

            // â”€â”€ Run scenarios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            return runScenarios(sub, runner, context);
        },
    };
}

// â”€â”€â”€ Sub-command handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function handleList(runner: TestRunner, context: CommandContext): CommandResult {
    const scenarios = runner.listScenarios();

    const byGroup = new Map<string, typeof scenarios>();
    for (const s of scenarios) {
        if (!byGroup.has(s.group)) { byGroup.set(s.group, []); }
        byGroup.get(s.group)!.push(s);
    }

    const lines: string[] = [
        '## ðŸ§ª Available Test Scenarios',
        '',
        `**Total:** ${scenarios.length} scenarios across ${byGroup.size} groups`,
        '',
    ];

    for (const [group, scenes] of byGroup) {
        lines.push(`### \`${group}\` â€” ${scenes.length} scenario(s)`, '');
        lines.push('| ID | Description | LLM |');
        lines.push('|----|-------------|:---:|');
        for (const s of scenes) {
            lines.push(`| \`${s.id}\` | ${s.description} | ${s.requiresLLM ? 'ðŸ¤–' : 'â€”'} |`);
        }
        lines.push('');
    }

    lines.push(
        '---',
        '',
        '**Quick Reference:**',
        '| Sub-command | Groups included |',
        '|-------------|-----------------|',
        '| `quick` | system, template, cmd |',
            '| `full` | system, template, cmd, agent, workflow, e2e |',
        '| `system` | system |',
        '| `templates` | template |',
        '| `commands` | cmd |',
        '| `agents` | agent |',
        '| `workflow` | workflow |',
            '| `e2e` | e2e |',
        '| `<scenario-id>` | single scenario |',
    );

    context.stream.markdown(lines.join('\n'));
    return { success: true, message: 'Scenario list displayed' };
}

async function runScenarios(
    subCommand: string,
    runner: TestRunner,
    context: CommandContext
): Promise<CommandResult> {
    const startTime = new Date();
    const runId = `run-${startTime.getTime()}`;
    const modelName =
        (context.model as unknown as Record<string, unknown> | undefined)?.['name'] as string
        ?? (context.model as unknown as Record<string, unknown> | undefined)?.['id'] as string
        ?? 'unknown';

    // â”€â”€ Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    context.stream.markdown([
        `## ðŸ§ª \`/test ${subCommand}\``,
        '',
        `| Property | Value |`,
        `|----------|-------|`,
        `| Run ID | \`${runId}\` |`,
        `| Model | **${modelName}** |`,
        `| Time | ${startTime.toLocaleTimeString()} |`,
        '',
        '---',
        '',
        '**Running scenariosâ€¦**',
        '',
    ].join('\n'));

    // â”€â”€ Execute â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const results = await runner.run(subCommand, context, (msg) => {
        context.stream.markdown(`\n${msg}`);
    });

    if (results.length === 0) {
        context.stream.markdown([
            '',
            `---`,
            '',
            `â“ No scenarios matched \`${subCommand}\`.`,
            'Use `/test list` to see all available scenarios and group names.',
        ].join('\n'));
        return { success: false, error: `No scenarios found for sub-command: '${subCommand}'` };
    }

    // â”€â”€ Summarise â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const endTime   = new Date();
    const durationMs = endTime.getTime() - startTime.getTime();
    const summary    = buildSummary(results);

    const testRun: TestRunResult = {
        runId,
        startTime: startTime.toISOString(),
        endTime:   endTime.toISOString(),
        totalDurationMs: durationMs,
        subCommand,
        model: modelName,
        workspaceRoot: context.workspaceRoot,
        summary,
        scenarios: results,
    };

    // â”€â”€ Persist logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const saved = await TestLogger.save(testRun);

    // â”€â”€ Render chat summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    context.stream.markdown('\n---\n');
    context.stream.markdown(renderSummaryMarkdown(testRun, saved));

    return {
        success: summary.failed === 0 && summary.errors === 0,
        data: testRun,
    };
}

// â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function buildSummary(results: ScenarioResult[]): TestRunResult['summary'] {
    return {
        total:   results.length,
        passed:  results.filter(r => r.status === 'passed').length,
        failed:  results.filter(r => r.status === 'failed').length,
        errors:  results.filter(r => r.status === 'error').length,
        skipped: results.filter(r => r.status === 'skipped').length,
    };
}

function renderSummaryMarkdown(
    r: TestRunResult,
    saved: { jsonPath: string; mdPath: string } | null
): string {
    const passRate = r.summary.total > 0
        ? Math.round(((r.summary.passed + r.summary.skipped) / r.summary.total) * 100)
        : 0;

    const allGood = r.summary.failed === 0 && r.summary.errors === 0;

    const verdict = allGood
        ? `âœ… **All scenarios passed** (${passRate}% pass rate)`
        : `âš ï¸ **${r.summary.failed + r.summary.errors} scenario(s) need attention** (${passRate}% pass rate)`;

    const lines: string[] = [
        `## ðŸ“Š Results â€” \`/test ${r.subCommand}\``,
        '',
        verdict,
        '',
        `| Total | âœ… Passed | âŒ Failed | ðŸ’¥ Errors | â­ï¸ Skipped | â± Duration |`,
        `|-------|----------|----------|----------|----------|-----------|`,
        `| ${r.summary.total} | **${r.summary.passed}** | **${r.summary.failed}** | **${r.summary.errors}** | **${r.summary.skipped}** | ${r.totalDurationMs}ms |`,
        '',
    ];

    if (saved) {
        lines.push(
            `ðŸ“ **Logs saved to workspace:**`,
            `- Markdown report: \`${saved.mdPath}\``,
            `- JSON data:       \`${saved.jsonPath}\``,
            '',
        );
    }

    const failures = r.scenarios.filter(s => s.status === 'failed' || s.status === 'error');
    if (failures.length > 0) {
        lines.push('### âŒ Failures & Errors', '');
        for (const f of failures) {
            const icon = f.status === 'error' ? 'ðŸ’¥' : 'âŒ';
            lines.push(`${icon} **\`${f.id}\`** â€” ${f.description}`);
            if (f.error)   { lines.push(`> Error: \`${f.error}\``); }
            if (f.details) { lines.push(`> ${f.details}`); }
            lines.push('');
        }
        lines.push(
            '### ðŸ’¡ Debugging Tips',
            '- Run `/test <scenario-id>` to isolate and re-run a single failing scenario',
            '- Check the full log report for stack traces or additional context',
            '- Run `/test quick` for the fastest feedback loop (no LLM calls)',
            '',
        );
    }

    return lines.join('\n');
}

// â”€â”€â”€ Help text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const HELP_TEXT = `## ðŸ§ª Docu Test Suite â€” \`/test\`

Automatically runs all extension scenarios using the currently selected GitHub Copilot model.
Results are streamed live to chat and saved to \`.docu/test-results/\` as JSON + Markdown.

### Sub-commands

| Command | Scenarios | LLM |
|---------|-----------|:---:|
| \`/test quick\` | system + templates + commands (fast) | â€” |
| \`/test system\` | Agents registered, state load/save, template count | â€” |
| \`/test templates\` | Render all 5 built-in templates | â€” |
| \`/test commands\` | All command-routing scenarios | â€” |
| \`/test agents\` | All 6 agents respond via the selected LLM | ðŸ¤– |
| \`/test workflow\` | Phase tracking + /prd /requirements /design /spec /review | ðŸ¤– |
| \`/test e2e\` | **Full spec workflow** â€” creates real files in \`.docu/test-e2e/\` | ðŸ¤– |
| \`/test full\` | **Everything** â€” all groups including e2e | ðŸ¤– |
| \`/test list\` | List every scenario and its group | â€” |
| \`/test <id>\` | Run a single scenario, e.g. \`/test e2e:prd-creation\` | varies |

### Log files

After every run, two files are written to \`.docu/test-results/\`:
- \`test-<timestamp>.json\` â€” full machine-readable results
- \`test-<timestamp>.md\`   â€” human-readable Markdown report

### Examples

\`\`\`
/test quick
/test full
/test e2e
/test agents
/test e2e:prd-creation
/test agent:quality-reviewer
/test system:agent-registration
\`\`\`
`;
